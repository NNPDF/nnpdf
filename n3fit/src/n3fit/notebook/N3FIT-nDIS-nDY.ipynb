{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c613114c",
   "metadata": {},
   "source": [
    "# Extend N3FIT to integrate nDIS/nDY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d11b87",
   "metadata": {},
   "source": [
    "The following notebook replicates the main parts of the **n3fit** fitting code. Its main purpose is to serve as a playground to implement various (subtle) features such as the inclusion of nuclear fits. Similar to the main fitting code, we rely on the `n3fit.backends` to perform various operations. As an important information, this notebook also relies on a modified version of some parts of the main code, these changes mainly affect the `backends` and the `layers` modules. Incrementally, we are going to complete the **nDIS** part first and then the **nDY**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1880c2",
   "metadata": {},
   "source": [
    "## 1. Short introduction\n",
    "\n",
    "Most of the nuclear (Neutral Current, or in short NC) DIS datasets are given as ration of structure functions with different nuclei:\n",
    "$$ \\mathcal{O} (x, A_1, A_2, Q^2) = \\frac{F_2 (x, A_1, Q^2)}{F_2 (x, A_2, Q^2)} \\quad \\mathrm{with} \\quad F_2 (x, A, Q^2) = \\sum^{n_f}_{i} \\sum^{n_x}_{\\alpha} \\mathrm{FK}_{ij} (x, x_\\alpha, Q^2, Q^2_0) f_i^A (x, Q^2_0) $$\n",
    "where $A$ denotes the atomic mass number, $f^A$ denotes the bound-nucleon PDF for a nucleus with atomic number $A$, and the rest carries the usual meaning. It is important to emphasize that the **FK** tables that appear in the numerator and in the denominator are the same. In turns, the bound-nucleon PDFs $f_i^A$ at a momentum fraction $x$ and scale $Q^2_0$ are expressed in terms of the bound-proton PDFs $f_i^{p/A}$ and bound-neutron PDFs $f_i^{n/A}$ as follows:\n",
    "$$ f_i^A (x, Q^2_0) = Z f_i^{p/A} (x, Q^2_0) + (A-Z) f_i^{n/A} (x, Q^2_0). $$\n",
    "The bound-proton and bound-neutron PDFs are related by **isospin asymmetry** via the following relations:\n",
    "$$ u^{p/A}(x, Q^2_0) = d^{n/A}(x, Q^2_0), \\: d^{p/A}(x, Q^2_0) = u^{n/A}(x, Q^2_0), \\: \\bar{u}^{p/A}(x, Q^2_0) = \\bar{d}^{n/A}(x, Q^2_0), \\: \\bar{d}^{p/A}(x, Q^2_0) = \\bar{u}^{n/A}(x, Q^2_0) $$\n",
    "and $f_i^{p/A} = f_i^{n/A}$ for other PDF flavours. In practice, one fits the bound-proton PDFs in which constraints such as **sum rules** can be imposed.\n",
    "\n",
    "#### Evolution basis version:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef10b8",
   "metadata": {},
   "source": [
    "## 2. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc6bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras backend\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "from abc import abstractmethod, ABC\n",
    "from dataclasses import dataclass\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Use n3fit backends which are wrappers around\n",
    "# tf.keras backends.\n",
    "from n3fit.backends import Input\n",
    "from n3fit.backends import base_layer_selector\n",
    "from n3fit.backends import MetaModel\n",
    "from n3fit.backends import callbacks\n",
    "from n3fit.backends import MetaLayer\n",
    "from n3fit.backends import operations as op\n",
    "from n3fit.backends import clear_backend_state\n",
    "\n",
    "from n3fit.stopping import Stopping\n",
    "from n3fit.msr import msr_impose\n",
    "\n",
    "from n3fit.layers import DIS, DY\n",
    "from n3fit.layers import ObsRotation\n",
    "from n3fit.layers import losses\n",
    "from n3fit.layers import Preprocessing\n",
    "from n3fit.layers import FkRotation\n",
    "from n3fit.layers import FlavourToEvolution\n",
    "from n3fit.backends import MetaLayer, Lambda\n",
    "from n3fit.backends import base_layer_selector\n",
    "from n3fit.backends import regularizer_selector\n",
    "\n",
    "# Define seeds\n",
    "random.seed(123)\n",
    "np.random.seed(456)\n",
    "console = Console()\n",
    "\n",
    "from n3fit.vpinterface import N3PDF\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca600c",
   "metadata": {},
   "source": [
    "## 3. Load toy-datasets and Add $A$-dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b76845",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we use as inputs saved files (`toyexpinfo.pkl` for experimental datasets, `posdatasets.pkl` for positivity datasets, and `integdatasets.pkl` for integrability datasets) generated from a **n3fit** run using the `toy-runcard.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ce68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_pkl_file = open(\"global-toyexpinfo.pkl\", \"rb\")\n",
    "toyexpinfo = pickle.load(exp_pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f834749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pkl_file = open(\"global-posdatasets.pkl\", \"rb\")\n",
    "toyposdatasets = pickle.load(pos_pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1f4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "integ_pkl_file = open(\"global-integdatasets.pkl\", \"rb\")\n",
    "toyintegdatasets = pickle.load(integ_pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6082c3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEUTERON', 'NUCLEAR', 'CMS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[expgroup['name'] for expgroup in toyexpinfo]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375b872",
   "metadata": {},
   "source": [
    "Now, we need to **add** the $A$-dependence to the various datasets to be passed along the FK tables for training. One still needs to think about how to include such information in **n3fit/validphys**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3295cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following maps a dataset to the corresponding atomic numbers. For DIS, if\n",
    "the list is lenght two (RATIO), the first element represent the A of the numerator\n",
    "while the second element the A in the denominator. The Hadronic case is similar\n",
    "with the subtlelty that the elements in the list only denote what is being collided\n",
    "with the proton.\n",
    "\"\"\"\n",
    "map_datasets_to_as = {\n",
    "    \"NMCPD_dw\": [1, 1],\n",
    "    \"SLACP_dwsh\": [1],\n",
    "    \"NMC_PB_C\": [208, 12],\n",
    "    \"NMC_BE_C\": [9, 12],\n",
    "    \"CMS_1JET_8TEV\": [1],\n",
    "    \"CMS_pPb_2JET_5TEV\": [208, 1],\n",
    "    \"CMS_pPb_WM_8TEV\": [208]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8494131",
   "metadata": {},
   "source": [
    "For the purpose of this test we only add the $A$-dependence to the `experimental` datasets. The `positivity` (and `integrability`) datasets are not fully ready yet. For this reason, only the positivity and integrability of the free proton PDFs are checked during the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ebdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_A_dependence(expinfo: list) -> list:\n",
    "    \"\"\"Takes the usual inputs for n3fit (raw datasets from validphys/NNPDF)\n",
    "    and add the A-values to the dictionaries. The following should be added\n",
    "    to validphys/n3fit somehow, for examples, through the input run card.\n",
    "    \n",
    "    This is just for testing purposes, ie. not physical meanings at all.\n",
    "    \"\"\"\n",
    "    for dataset_group in expinfo:\n",
    "        for dataset in dataset_group[\"datasets\"]:\n",
    "            if dataset[\"name\"] in map_datasets_to_as.keys():\n",
    "                dataset['A'] = map_datasets_to_as[dataset[\"name\"]]\n",
    "            else:\n",
    "                raise ValueError(\"Dataset not recognised. Check!\")\n",
    "    return expinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc6c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_active_A(expinfo: list) -> list:\n",
    "    \"\"\"Take the new list from `add_A_dependence` in which the information\n",
    "    on the atomic mass number A is included and returns an order list of\n",
    "    the A included in the fit.\n",
    "    \"\"\"\n",
    "    A_lists = []\n",
    "    for dataset_group in expinfo:\n",
    "        for dataset in dataset_group[\"datasets\"]:\n",
    "            A_lists.append(dataset[\"A\"])\n",
    "    merged = [item for sublist in A_lists for item in sublist]\n",
    "    return sorted(list(set(merged)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac890c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_expinfo(expinfo: list) -> None:\n",
    "    \"\"\"Summarize the information concerning the atomic mass number\n",
    "    for the datasets included in the actual fitting playgrounds.\n",
    "    \"\"\"\n",
    "    table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(\"Dataset\", justify=\"left\", width=24)\n",
    "    table.add_column(\"A1\", justify=\"left\", width=24)\n",
    "    table.add_column(\"A2\", justify=\"left\", width=24)\n",
    "    for dataset_group in expinfo:\n",
    "        for dataset in dataset_group[\"datasets\"]:\n",
    "            A1_value = dataset['A'][0]\n",
    "            A2_value = dataset['A'][1] if len(dataset['A']) == 2 else None\n",
    "            table.add_row(f\"{dataset['name']}\", f\"{A1_value}\", f\"{A2_value}\")\n",
    "    console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca43601",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_toyexpinfo = add_A_dependence(toyexpinfo)\n",
    "LIST_OF_FITTED_A = list_active_A(new_toyexpinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fbd5a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Dataset                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> A1                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> A2                       </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ NMCPD_dw                 │ 1                        │ 1                        │\n",
       "│ SLACP_dwsh               │ 1                        │ None                     │\n",
       "│ NMC_PB_C                 │ 208                      │ 12                       │\n",
       "│ NMC_BE_C                 │ 9                        │ 12                       │\n",
       "│ CMS_pPb_2JET_5TEV        │ 208                      │ 1                        │\n",
       "│ CMS_pPb_WM_8TEV          │ 208                      │ None                     │\n",
       "│ CMS_1JET_8TEV            │ 1                        │ None                     │\n",
       "└──────────────────────────┴──────────────────────────┴──────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mDataset                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mA1                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mA2                      \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ NMCPD_dw                 │ 1                        │ 1                        │\n",
       "│ SLACP_dwsh               │ 1                        │ None                     │\n",
       "│ NMC_PB_C                 │ 208                      │ 12                       │\n",
       "│ NMC_BE_C                 │ 9                        │ 12                       │\n",
       "│ CMS_pPb_2JET_5TEV        │ 208                      │ 1                        │\n",
       "│ CMS_pPb_WM_8TEV          │ 208                      │ None                     │\n",
       "│ CMS_1JET_8TEV            │ 1                        │ None                     │\n",
       "└──────────────────────────┴──────────────────────────┴──────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">List of fitted As: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">208</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mList of fitted As: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;36m, \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;36m, \u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;36m, \u001b[0m\u001b[1;36m208\u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_expinfo(new_toyexpinfo)\n",
    "console.print(f\"List of fitted As: {LIST_OF_FITTED_A}\", style=\"bold cyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62078757",
   "metadata": {},
   "source": [
    "## 4. Create the NN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464fef9",
   "metadata": {},
   "source": [
    "### 4.1 Construct the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7e0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_network(\n",
    "    nodes_in,\n",
    "    nodes,\n",
    "    activations,\n",
    "    initializer_name=\"glorot_normal\",\n",
    "    As_number=1,\n",
    "    seed=0,\n",
    "    dropout_rate=0.0,\n",
    "    regularizer=None,\n",
    "):\n",
    "    list_of_pdf_layers = []\n",
    "    # Modifications: Multiply the number of nodes in the last layer\n",
    "    # with the number of active As involved in the fitting procedure.\n",
    "    nodes[-1] *= As_number\n",
    "    number_of_layers = len(nodes)\n",
    "    if dropout_rate > 0:\n",
    "        dropout_layer = number_of_layers - 2\n",
    "    else:\n",
    "        dropout_layer = -1\n",
    "    for i, (nodes_out, activation) in enumerate(zip(nodes, activations)):\n",
    "        if dropout_rate > 0 and i == dropout_layer:\n",
    "            list_of_pdf_layers.append(base_layer_selector(\"dropout\", rate=dropout_rate))\n",
    "        init = MetaLayer.select_initializer(initializer_name, seed=seed + i)\n",
    "        arguments = {\n",
    "            \"kernel_initializer\": init,\n",
    "            \"units\": int(nodes_out),\n",
    "            \"activation\": activation,\n",
    "            \"input_shape\": (nodes_in,),\n",
    "            \"kernel_regularizer\": regularizer,\n",
    "        }\n",
    "        layer = base_layer_selector(\"dense\", **arguments)\n",
    "        list_of_pdf_layers.append(layer)\n",
    "        nodes_in = int(nodes_out)\n",
    "\n",
    "    return list_of_pdf_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc92da",
   "metadata": {},
   "source": [
    "### 4.3 Construct the complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da632528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfNN_layer_generator(\n",
    "    inp=2,\n",
    "    nodes=None,\n",
    "    activations=None,\n",
    "    initializer_name=\"glorot_normal\",\n",
    "    As_number=1,\n",
    "    layer_type=\"dense\",\n",
    "    flav_info=None,\n",
    "    fitbasis=\"NN31IC\",\n",
    "    out=14,\n",
    "    seed=None,\n",
    "    dropout=0.0,\n",
    "    regularizer=None,\n",
    "    regularizer_args=None,\n",
    "    impose_sumrule=None,\n",
    "    scaler=None,\n",
    "    parallel_models=1,\n",
    "):\n",
    "    \"\"\"In case of proton fit, this function acts in the standard way. In case A!=1,\n",
    "    further extensions had to be implemented. Recall that the output of the\n",
    "    `generate_dens_network` has a dimension (FITTING_BASIS_SIZE*As_number). Similar\n",
    "    operations as in the proton fit therefore applies to each individual A involved \n",
    "    in the fit. As a result, various custom layers had to be extended to take this\n",
    "    into consideration.\n",
    "    \n",
    "        * Modified layers so far: FKRotation, FlavourToEvolution, msr_impose\n",
    "        * Stil needs to be modified: preprocessing\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        As_number: int\n",
    "            Number of active A's (nb of A involved in the fit.)\n",
    "            \n",
    "        Missing:\n",
    "        -------\n",
    "        At this point, what is still missing (apart from preprocessing) is the part\n",
    "        that really computes the nPDFs as a combination of bound-proton and bound\n",
    "        neutron PDFs. Not sure yet if this should be implemented as a layer here or\n",
    "        computed at the observable levels.\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = parallel_models * [None]\n",
    "    elif isinstance(seed, int):\n",
    "        seed = parallel_models * [seed]\n",
    "\n",
    "    if nodes is None:\n",
    "        nodes = [15, 8]\n",
    "    ln = len(nodes)\n",
    "\n",
    "    if impose_sumrule is None:\n",
    "        impose_sumrule = \"All\"\n",
    "\n",
    "    if scaler:\n",
    "        inp = 1\n",
    "\n",
    "    if activations is None:\n",
    "        activations = [\"tanh\", \"linear\"]\n",
    "    elif callable(activations):\n",
    "        activations = activations(ln)\n",
    "\n",
    "    if regularizer_args is None:\n",
    "        regularizer_args = dict()\n",
    "\n",
    "    number_of_layers = len(nodes)\n",
    "    last_layer_nodes = nodes[-1]  \n",
    "    \n",
    "    placeholder_input = Input(shape=(None, 1), batch_size=1)\n",
    "\n",
    "    subtract_one = False\n",
    "    process_input = Lambda(lambda x: x)\n",
    "    input_x_eq_1 = [1.0]\n",
    "    if scaler:\n",
    "        process_input = Lambda(lambda x: 2 * x - 1)\n",
    "        subtract_one = True\n",
    "        input_x_eq_1 = scaler([1.0])[0]\n",
    "        placeholder_input = Input(shape=(None, 2), batch_size=1)\n",
    "    elif inp == 2:\n",
    "        process_input = Lambda(lambda x: op.concatenate([x, op.op_log(x)], axis=-1))\n",
    "\n",
    "    model_input = [placeholder_input]\n",
    "    if subtract_one:\n",
    "        layer_x_eq_1 = op.numpy_to_input(np.array(input_x_eq_1).reshape(1, 1))\n",
    "        model_input.append(layer_x_eq_1)\n",
    "\n",
    "    layer_evln = FkRotation(input_shape=(last_layer_nodes,), output_dim=out)\n",
    "    basis_rotation = FlavourToEvolution(flav_info=flav_info, fitbasis=fitbasis)\n",
    "\n",
    "    if impose_sumrule:\n",
    "        sumrule_layer, integrator_input = msr_impose(mode=impose_sumrule, scaler=scaler)\n",
    "        model_input.append(integrator_input)\n",
    "    else:\n",
    "        sumrule_layer = lambda x: x\n",
    "\n",
    "    pdf_models = []\n",
    "    for i, layer_seed in enumerate(seed):\n",
    "        if layer_type == \"dense\":\n",
    "            reg = regularizer_selector(regularizer, **regularizer_args)\n",
    "            list_of_pdf_layers = generate_dense_network(\n",
    "                inp,\n",
    "                nodes,\n",
    "                activations,\n",
    "                initializer_name,\n",
    "                As_number=As_number,\n",
    "                seed=layer_seed,\n",
    "                dropout_rate=dropout,\n",
    "                regularizer=reg,\n",
    "            )\n",
    "        elif layer_type == \"dense_per_flavour\":\n",
    "            list_of_pdf_layers = generate_dense_per_flavour_network(\n",
    "                inp,\n",
    "                nodes,\n",
    "                activations,\n",
    "                initializer_name,\n",
    "                seed=layer_seed,\n",
    "                basis_size=last_layer_nodes,\n",
    "            )\n",
    "\n",
    "        def dense_me(x):\n",
    "            \"\"\"Takes an input tensor `x` and applies all layers\n",
    "            from the `list_of_pdf_layers` in order\"\"\"\n",
    "            processed_x = process_input(x)\n",
    "            curr_fun = list_of_pdf_layers[0](processed_x)\n",
    "\n",
    "            for dense_layer in list_of_pdf_layers[1:]:\n",
    "                curr_fun = dense_layer(curr_fun)\n",
    "            return curr_fun\n",
    "\n",
    "        preproseed = layer_seed + number_of_layers\n",
    "        layer_preproc = Preprocessing(\n",
    "            flav_info=flav_info,\n",
    "            As_number=As_number,\n",
    "            input_shape=(1,),\n",
    "            name=f\"pdf_prepro_{i}\",\n",
    "            seed=preproseed,\n",
    "            large_x=not subtract_one,\n",
    "        )\n",
    "\n",
    "        def layer_fitbasis(x):\n",
    "            x_scaled = op.op_gather_keep_dims(x, 0, axis=-1)\n",
    "            x_original = op.op_gather_keep_dims(x, -1, axis=-1)\n",
    "\n",
    "            nn_output = dense_me(x_scaled)\n",
    "            if subtract_one:\n",
    "                nn_at_one = dense_me(layer_x_eq_1)\n",
    "                nn_output = op.op_subtract([nn_output, nn_at_one])\n",
    "\n",
    "            # Ignore Preprocessing for the Time Being. Still thinking of\n",
    "            # The best way to take preprocessing into account, whether a\n",
    "            # same flavour for different A's should be the same.\n",
    "            ret = op.op_multiply([nn_output, layer_preproc(x_original)])\n",
    "            # ret = nn_output\n",
    "            if basis_rotation.is_identity():\n",
    "                return ret\n",
    "            return basis_rotation(ret)\n",
    "\n",
    "        def layer_pdf(x):\n",
    "            return layer_evln(layer_fitbasis(x))\n",
    "\n",
    "        final_pdf = sumrule_layer(layer_pdf)\n",
    "\n",
    "        pdf_model = MetaModel(\n",
    "            model_input, final_pdf(placeholder_input), name=f\"PDF_{i}\", scaler=scaler\n",
    "        )\n",
    "        pdf_models.append(pdf_model)\n",
    "    return pdf_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836de23",
   "metadata": {},
   "source": [
    "### 4.4 Construct the Observable\n",
    "\n",
    "Now, we can implement the part that computes the Observable ($\\mathcal{O}^{\\rm th}$) expressed in the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59c6387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_unique(list_of_arrays):\n",
    "    \"\"\" Check whether the list of arrays more than one different arrays \"\"\"\n",
    "    the_first = list_of_arrays[0]\n",
    "    for i in list_of_arrays[1:]:\n",
    "        if not np.array_equal(the_first, i):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "class Observable(MetaLayer, ABC):\n",
    "\n",
    "    def __init__(self, fktable_dicts, fktable_arr, operation_name, A_list, nfl=14, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Modifications:\n",
    "        --------------\n",
    "            Inputs:\n",
    "            -------\n",
    "            A_list:\n",
    "                List containing the information on the atomic number for a given\n",
    "                dataset. The dimension of the list should be the same as the dim\n",
    "                of the `fktable_arr`. For example, if the `operation_name` is a\n",
    "                `RATIO`, ie the `fktable_arr` is containing 2 elements, then the\n",
    "                `A_list` is a list of two elements each representing the atomic\n",
    "                number for the numerator and the denominator.\n",
    "        \"\"\"\n",
    "        super(MetaLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.nfl = nfl\n",
    "\n",
    "        basis = []\n",
    "        xgrids = []\n",
    "        self.fktables = []\n",
    "        # Add the information on the atomic number A as an\n",
    "        # attribute. Its length should be the same as the\n",
    "        # `fktable_dicts` or `fktable_arr`.\n",
    "        self.A_list = A_list\n",
    "        for fktable, fk in zip(fktable_dicts, fktable_arr):\n",
    "            xgrids.append(fktable[\"xgrid\"])\n",
    "            basis.append(fktable[\"basis\"])\n",
    "            self.fktables.append(op.numpy_to_tensor(fk))\n",
    "\n",
    "        if _is_unique(xgrids):\n",
    "            self.splitting = None\n",
    "        else:\n",
    "            self.splitting = [i.shape[1] for i in xgrids]\n",
    "\n",
    "        if _is_unique(basis) and _is_unique(xgrids):\n",
    "            self.all_masks = [self.gen_mask(basis[0])]\n",
    "            self.many_masks = False\n",
    "        else:\n",
    "            self.many_masks = True\n",
    "            self.all_masks = [self.gen_mask(i) for i in basis]\n",
    "\n",
    "        self.operation = op.c_to_py_fun(operation_name)\n",
    "        self.output_dim = self.fktables[0].shape[0]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (self.output_dim, None)\n",
    "\n",
    "    @abstractmethod\n",
    "    def gen_mask(self, basis):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89dd6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIS(Observable):\n",
    "\n",
    "    def gen_mask(self, basis):\n",
    "\n",
    "        if basis is None:\n",
    "            self.basis = np.ones(self.nfl, dtype=bool)\n",
    "        else:\n",
    "            basis_mask = np.zeros(self.nfl, dtype=bool)\n",
    "            for i in basis:\n",
    "                basis_mask[i] = True\n",
    "        return op.numpy_to_tensor(basis_mask, dtype=bool)\n",
    "\n",
    "    def call(self, pdf):\n",
    "        \"\"\"\n",
    "        Modifications:\n",
    "        --------------\n",
    "        The input `pdf` now has a dimension `self.nfl*As_number`\n",
    "        \"\"\"\n",
    "\n",
    "        if self.splitting is not None:\n",
    "            raise ValueError(\"DIS layer call with a dataset that needs more than one xgrid?\")\n",
    "\n",
    "        results = []\n",
    "        # Now we should map the outputs of the NNs to the corresponding nucleus. The operation \n",
    "        # will be applied at the very end.\n",
    "        # TODO: Find a better way to propagate the ordered list of active A in the computation \n",
    "        # of observable.\n",
    "        # TODO: Fix the ways in which the POSITIVITY & INTEGRABILITY datasets are handled below\n",
    "        if self.many_masks:\n",
    "            for a, (mask, fktable) in enumerate(zip(self.all_masks, self.fktables)):\n",
    "                output_index = 0 if self.A_list is None else LIST_OF_FITTED_A.index(self.A_list[a])\n",
    "                split_size = int(pdf.shape[2] / self.nfl)\n",
    "                splitted_pdf = op.split(pdf, num_or_size_splits=split_size, axis=2)\n",
    "                output_pdf_range = splitted_pdf[output_index]\n",
    "                pdf_masked = op.boolean_mask(output_pdf_range, mask, axis=2)\n",
    "                res = op.tensor_product(pdf_masked, fktable, axes=[(1, 2), (2, 1)])\n",
    "                results.append(res)\n",
    "        else:\n",
    "            for a, fktable in enumerate(self.fktables):\n",
    "                output_index = 0 if self.A_list is None else LIST_OF_FITTED_A.index(self.A_list[a])\n",
    "                split_size = int(pdf.shape[2] / self.nfl)\n",
    "                splitted_pdf = op.split(pdf, num_or_size_splits=split_size, axis=2)\n",
    "                output_pdf_range = splitted_pdf[output_index]\n",
    "                pdf_masked = op.boolean_mask(output_pdf_range, self.all_masks[0], axis=2)\n",
    "                res = op.tensor_product(pdf_masked, fktable, axes=[(1, 2), (2, 1)])\n",
    "                results.append(res)\n",
    "        # Finally apply the operations\n",
    "        final_results = self.operation(results)\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f63bb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_obs = DIS(\n",
    "    toyexpinfo[0]['datasets'][0]['fktables'], \n",
    "    toyexpinfo[0]['datasets'][0]['tr_fktables'],\n",
    "    toyexpinfo[0]['datasets'][0]['operation'],\n",
    "    toyexpinfo[0]['datasets'][0]['A']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff972f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DY(Observable):\n",
    "    \n",
    "    def gen_mask(self, basis):\n",
    "        if basis is None:\n",
    "            basis_mask = np.ones((self.nfl, self.nfl), dtype=bool)\n",
    "        else:\n",
    "            basis_mask = np.zeros((self.nfl, self.nfl), dtype=bool)\n",
    "            for i, j in basis.reshape(-1, 2):\n",
    "                basis_mask[i, j] = True\n",
    "        return op.numpy_to_tensor(basis_mask, dtype=bool)\n",
    "\n",
    "    def call(self, pdf_raw):\n",
    "        \"\"\"\n",
    "        Modifications:\n",
    "        --------------\n",
    "        The input `pdf` now has a dimension `self.nfl*As_number`\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        if self.many_masks:\n",
    "            if self.splitting:\n",
    "                # TODO: Avoid repeated computations by improving the loop below\n",
    "                for a, (mask, fk) in enumerate(zip(self.all_masks, self.fktables)):\n",
    "                    # First we need to split the PDFs to select the corresponding f^A\n",
    "                    output_index = 0 if self.A_list is None else LIST_OF_FITTED_A.index(self.A_list[a])\n",
    "                    split_size = int(pdf_raw.shape[2] / self.nfl)\n",
    "                    splitted_pdf = op.split(pdf_raw, num_or_size_splits=split_size, axis=2)\n",
    "                    nonsplitted_pdf1 = splitted_pdf[0]\n",
    "                    nonsplitted_pdf2 = splitted_pdf[output_index]\n",
    "                    # Then performing the standard splitting of a given f^A\n",
    "                    pdf1 = op.split(nonsplitted_pdf1, self.splitting, axis=1)[a]\n",
    "                    pdf2 = op.split(nonsplitted_pdf2, self.splitting, axis=1)[a]\n",
    "                    pdf_x_pdf = op.pdf_masked_convolution(pdf1, pdf2, mask)\n",
    "                    res = op.tensor_product(fk, pdf_x_pdf, axes=3)\n",
    "                    results.append(res)\n",
    "            else:\n",
    "                for a, (mask, fk) in enumerate(zip(self.all_masks, self.fktables)):\n",
    "                    output_index = 0 if self.A_list is None else LIST_OF_FITTED_A.index(self.A_list[a])\n",
    "                    split_size = int(pdf_raw.shape[2] / self.nfl)\n",
    "                    splitted_pdf = op.split(pdf_raw, num_or_size_splits=split_size, axis=2)\n",
    "                    pdf1 = splitted_pdf[0]\n",
    "                    pdf2 = splitted_pdf[output_index]\n",
    "                    pdf_x_pdf = op.pdf_masked_convolution(pdf1, pdf2, mask)\n",
    "                    res = op.tensor_product(fk, pdf_x_pdf, axes=3)\n",
    "                    results.append(res)\n",
    "        else:\n",
    "            for a, fk in enumerate(self.fktables):\n",
    "                output_index = 0 if self.A_list is None else LIST_OF_FITTED_A.index(self.A_list[a])\n",
    "                split_size = int(pdf_raw.shape[2] / self.nfl)\n",
    "                splitted_pdf = op.split(pdf_raw, num_or_size_splits=split_size, axis=2)\n",
    "                pdf1 = splitted_pdf[0]\n",
    "                pdf2 = splitted_pdf[output_index]\n",
    "                pdf_x_pdf = op.pdf_masked_convolution(pdf1, pdf2, self.all_masks[0])\n",
    "                res = op.tensor_product(fk, pdf_x_pdf, axes=3)\n",
    "                results.append(res)\n",
    "\n",
    "        ret = op.transpose(self.operation(results))\n",
    "        return op.batchit(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff06ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from n3fit.layers.losses import LossPositivity\n",
    "from n3fit.layers.losses import LossInvcovmat\n",
    "from n3fit.layers.losses import LossIntegrability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21ef96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ObservableWrapper:\n",
    "\n",
    "    name: str\n",
    "    observables: list\n",
    "    dataset_xsizes: list\n",
    "    invcovmat: np.array = None\n",
    "    covmat: np.array = None\n",
    "    multiplier: float = 1.0\n",
    "    integrability: bool = False\n",
    "    positivity: bool = False\n",
    "    data: np.array = None\n",
    "    rotation: ObsRotation = None\n",
    "\n",
    "    def _generate_loss(self, mask=None):\n",
    "        \n",
    "        if self.invcovmat is not None:\n",
    "            loss = losses.LossInvcovmat(\n",
    "                self.invcovmat, self.data, mask, covmat=self.covmat, name=self.name\n",
    "            )\n",
    "        elif self.positivity:\n",
    "            loss = losses.LossPositivity(name=self.name, c=self.multiplier)\n",
    "        elif self.integrability:\n",
    "            loss = losses.LossIntegrability(name=self.name, c=self.multiplier)\n",
    "        return loss\n",
    "\n",
    "    def _generate_experimental_layer(self, pdf):\n",
    "        \n",
    "        if len(self.dataset_xsizes) > 1:\n",
    "            splitting_layer = op.as_layer(\n",
    "                op.split,\n",
    "                op_args=[self.dataset_xsizes],\n",
    "                op_kwargs={\"axis\": 1},\n",
    "                name=f\"{self.name}_split\",\n",
    "            )\n",
    "            split_pdf = splitting_layer(pdf)\n",
    "        else:\n",
    "            split_pdf = [pdf]\n",
    "        output_layers = [obs(p_pdf) for p_pdf, obs in zip(split_pdf, self.observables)]\n",
    "        ret = op.concatenate(output_layers, axis=2)\n",
    "        if self.rotation is not None:\n",
    "            ret = self.rotation(ret)\n",
    "        return ret\n",
    "\n",
    "    def __call__(self, pdf_layer, mask=None):\n",
    "        loss_f = self._generate_loss(mask)\n",
    "        experiment_prediction = self._generate_experimental_layer(pdf_layer)\n",
    "        return loss_f(experiment_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccf37310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observable_generator(\n",
    "    spec_dict, positivity_initial=1.0, integrability=False\n",
    "):\n",
    "    \n",
    "    spec_name = spec_dict[\"name\"]\n",
    "    dataset_xsizes = []\n",
    "    model_obs_tr = []\n",
    "    model_obs_vl = []\n",
    "    model_obs_ex = []\n",
    "    model_inputs = []\n",
    "\n",
    "    for dataset_dict in spec_dict[\"datasets\"]:\n",
    "        dataset_name = dataset_dict[\"name\"]\n",
    "\n",
    "        if dataset_dict[\"hadronic\"]:\n",
    "            Obs_Layer = DY\n",
    "        else:\n",
    "            Obs_Layer = DIS\n",
    "\n",
    "        operation_name = dataset_dict[\"operation\"]\n",
    "        if spec_dict[\"positivity\"]:\n",
    "            obs_layer_tr = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"tr_fktables\"],\n",
    "                operation_name,\n",
    "                dataset_dict.get(\"A\", None),\n",
    "                name=f\"dat_{dataset_name}\",\n",
    "            )\n",
    "            obs_layer_ex = obs_layer_vl = None\n",
    "        elif spec_dict.get(\"data_transformation_tr\") is not None:\n",
    "            obs_layer_ex = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"ex_fktables\"],\n",
    "                operation_name,\n",
    "                dataset_dict.get(\"A\", None),\n",
    "                name=f\"exp_{dataset_name}\",\n",
    "            )\n",
    "            obs_layer_tr = obs_layer_vl = obs_layer_ex\n",
    "        else:\n",
    "            obs_layer_tr = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"tr_fktables\"],\n",
    "                operation_name,\n",
    "                dataset_dict.get(\"A\", None),\n",
    "                name=f\"dat_{dataset_name}\",\n",
    "            )\n",
    "            obs_layer_ex = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"ex_fktables\"],\n",
    "                operation_name,\n",
    "                dataset_dict.get(\"A\", None),\n",
    "                name=f\"exp_{dataset_name}\",\n",
    "            )\n",
    "            obs_layer_vl = Obs_Layer(\n",
    "                dataset_dict[\"fktables\"],\n",
    "                dataset_dict[\"vl_fktables\"],\n",
    "                operation_name,\n",
    "                dataset_dict.get(\"A\", None),\n",
    "                name=f\"val_{dataset_name}\",\n",
    "            )\n",
    "\n",
    "        if obs_layer_tr.splitting is None:\n",
    "            xgrid = dataset_dict[\"fktables\"][0][\"xgrid\"]\n",
    "            model_inputs.append(xgrid)\n",
    "            dataset_xsizes.append(xgrid.shape[1])\n",
    "        else:\n",
    "            xgrids = [i[\"xgrid\"] for i in dataset_dict[\"fktables\"]]\n",
    "            model_inputs += xgrids\n",
    "            dataset_xsizes.append(sum([i.shape[1] for i in xgrids]))\n",
    "\n",
    "        model_obs_tr.append(obs_layer_tr)\n",
    "        model_obs_vl.append(obs_layer_vl)\n",
    "        model_obs_ex.append(obs_layer_ex)\n",
    "\n",
    "    full_nx = sum(dataset_xsizes)\n",
    "    if spec_dict[\"positivity\"]:\n",
    "        out_positivity = ObservableWrapper(\n",
    "            spec_name,\n",
    "            model_obs_tr,\n",
    "            dataset_xsizes,\n",
    "            multiplier=positivity_initial,\n",
    "            positivity=not integrability,\n",
    "            integrability=integrability,\n",
    "        )\n",
    "\n",
    "        layer_info = {\n",
    "            \"inputs\": model_inputs,\n",
    "            \"output_tr\": out_positivity,\n",
    "            \"experiment_xsize\": full_nx,\n",
    "        }\n",
    "        return layer_info\n",
    "\n",
    "    if spec_dict.get(\"data_transformation_tr\") is not None:\n",
    "        obsrot_tr = ObsRotation(spec_dict.get(\"data_transformation_tr\"))\n",
    "        obsrot_vl = ObsRotation(spec_dict.get(\"data_transformation_vl\"))\n",
    "    else:\n",
    "        obsrot_tr = None\n",
    "        obsrot_vl = None\n",
    "\n",
    "    out_tr = ObservableWrapper(\n",
    "        spec_name,\n",
    "        model_obs_tr,\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat\"],\n",
    "        data=spec_dict[\"expdata\"],\n",
    "        rotation=obsrot_tr,\n",
    "    )\n",
    "    out_vl = ObservableWrapper(\n",
    "        f\"{spec_name}_val\",\n",
    "        model_obs_vl,\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat_vl\"],\n",
    "        data=spec_dict[\"expdata_vl\"],\n",
    "        rotation=obsrot_vl,\n",
    "    )\n",
    "    out_exp = ObservableWrapper(\n",
    "        f\"{spec_name}_exp\",\n",
    "        model_obs_ex,\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat_true\"],\n",
    "        covmat=spec_dict[\"covmat\"],\n",
    "        data=spec_dict[\"expdata_true\"],\n",
    "        rotation=None,\n",
    "    )\n",
    "\n",
    "    layer_info = {\n",
    "        \"inputs\": model_inputs,\n",
    "        \"output\": out_exp,\n",
    "        \"output_tr\": out_tr,\n",
    "        \"output_vl\": out_vl,\n",
    "        \"experiment_xsize\": full_nx,\n",
    "    }\n",
    "    return layer_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954bfbc",
   "metadata": {},
   "source": [
    "What remains to do now is to combined everything and construct a class to perform a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb872f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "def _pdf_injection(pdf_layers, observables, masks):\n",
    "    \"\"\"Takes as input a list of PDF layers and if needed applies masks.\"\"\"\n",
    "    return [f(x, mask=m) for f, x, m in zip_longest(observables, pdf_layers, masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e6c97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUSH_POSITIVITY_EACH = 100\n",
    "PUSH_INTEGRABILITY_EACH = 100\n",
    "CHI2_THRESHOLD = 10.0\n",
    "\n",
    "def _LM_initial_and_multiplier(input_initial, input_multiplier, max_lambda, steps):\n",
    "    initial, multiplier = input_initial, input_multiplier\n",
    "    if multiplier is None:\n",
    "        if initial is None: initial = 1.0\n",
    "        multiplier = pow(max_lambda / initial, 1 / max(steps, 1))\n",
    "    elif initial is None:\n",
    "        initial = max_lambda / pow(multiplier, steps)\n",
    "    return initial, multiplier\n",
    "\n",
    "class ModelTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        exp_info,\n",
    "        pos_info,\n",
    "        integ_info,\n",
    "        flavinfo,\n",
    "        fitbasis,\n",
    "        nnseeds,\n",
    "        pass_status=\"ok\",\n",
    "        failed_status=\"fail\",\n",
    "        debug=False,\n",
    "        kfold_parameters=None,\n",
    "        max_cores=None,\n",
    "        model_file=None,\n",
    "        sum_rules=None,\n",
    "        parallel_models=1,\n",
    "    ):\n",
    "        \n",
    "        self.exp_info = exp_info\n",
    "        self.pos_info = pos_info\n",
    "        self.integ_info = integ_info\n",
    "        if self.integ_info is not None:\n",
    "            self.all_info = exp_info + pos_info + integ_info\n",
    "        else:\n",
    "            self.all_info = exp_info + pos_info\n",
    "        self.flavinfo = flavinfo\n",
    "        self.fitbasis = fitbasis\n",
    "        self._nn_seeds = nnseeds\n",
    "        self.pass_status = pass_status\n",
    "        self.failed_status = failed_status\n",
    "        self.debug = debug\n",
    "        self.all_datasets = []\n",
    "        self._scaler = None\n",
    "        self._parallel_models = parallel_models\n",
    "\n",
    "        if debug:\n",
    "            self.max_cores = 1\n",
    "        else:\n",
    "            self.max_cores = max_cores\n",
    "        self.model_file = model_file\n",
    "        self.print_summary = True\n",
    "        self.mode_hyperopt = False\n",
    "        self.impose_sumrule = sum_rules\n",
    "        self._hyperkeys = None\n",
    "        if kfold_parameters is None:\n",
    "            self.kpartitions = [None]\n",
    "            self.hyper_threshold = None\n",
    "        else:\n",
    "            self.kpartitions = kfold_parameters[\"partitions\"]\n",
    "            self.hyper_threshold = kfold_parameters.get(\"threshold\", HYPER_THRESHOLD)\n",
    "\n",
    "            penalties = kfold_parameters.get(\"penalties\", [])\n",
    "            self.hyper_penalties = []\n",
    "            for penalty in penalties:\n",
    "                pen_fun = getattr(n3fit.hyper_optimization.penalties, penalty)\n",
    "                self.hyper_penalties.append(pen_fun)\n",
    "                log.info(\"Adding penalty: %s\", penalty)\n",
    "\n",
    "            hyper_loss = kfold_parameters.get(\"target\", None)\n",
    "            if hyper_loss is None:\n",
    "                hyper_loss = \"average\"\n",
    "                log.warning(\"No minimization target selected, defaulting to '%s'\", hyper_loss)\n",
    "            log.info(\"Using '%s' as the target for hyperoptimization\", hyper_loss)\n",
    "            self._hyper_loss = getattr(n3fit.hyper_optimization.rewards, hyper_loss)\n",
    "\n",
    "        self.input_list = []\n",
    "        self.input_sizes = []\n",
    "        self.training = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"posdatasets\": [],\n",
    "            \"posmultipliers\": [],\n",
    "            \"posinitials\": [],\n",
    "            \"integdatasets\": [],\n",
    "            \"integmultipliers\": [],\n",
    "            \"integinitials\": [],\n",
    "            \"folds\": [],\n",
    "        }\n",
    "        self.validation = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"folds\": [],\n",
    "            \"posdatasets\": [],\n",
    "        }\n",
    "        self.experimental = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"folds\": [],\n",
    "        }\n",
    "\n",
    "        self._fill_the_dictionaries()\n",
    "\n",
    "        if self.validation[\"ndata\"] == 0:\n",
    "            self.no_validation = True\n",
    "            self.validation[\"expdata\"] = self.training[\"expdata\"]\n",
    "        else:\n",
    "            self.no_validation = False\n",
    "\n",
    "        self.callbacks = []\n",
    "        if debug:\n",
    "            self.callbacks.append(callbacks.TimerCallback())\n",
    "\n",
    "    def set_hyperopt(self, hyperopt_on, keys=None, status_ok=\"ok\"):\n",
    "        self.pass_status = status_ok\n",
    "        if keys is None:\n",
    "            keys = []\n",
    "        self._hyperkeys = keys\n",
    "        if hyperopt_on:\n",
    "            self.print_summary = False\n",
    "            self.mode_hyperopt = True\n",
    "        else:\n",
    "            self.print_summary = True\n",
    "            self.mode_hyperopt = False\n",
    "\n",
    "            \n",
    "    def _fill_the_dictionaries(self):\n",
    "        for exp_dict in self.exp_info:\n",
    "            self.training[\"expdata\"].append(exp_dict[\"expdata\"])\n",
    "            self.validation[\"expdata\"].append(exp_dict[\"expdata_vl\"])\n",
    "            self.experimental[\"expdata\"].append(exp_dict[\"expdata_true\"])\n",
    "\n",
    "            self.training[\"folds\"].append(exp_dict[\"folds\"][\"training\"])\n",
    "            self.validation[\"folds\"].append(exp_dict[\"folds\"][\"validation\"])\n",
    "            self.experimental[\"folds\"].append(exp_dict[\"folds\"][\"experimental\"])\n",
    "\n",
    "            nd_tr = exp_dict[\"ndata\"]\n",
    "            nd_vl = exp_dict[\"ndata_vl\"]\n",
    "\n",
    "            self.training[\"ndata\"] += nd_tr\n",
    "            self.validation[\"ndata\"] += nd_vl\n",
    "            self.experimental[\"ndata\"] += nd_tr + nd_vl\n",
    "\n",
    "            for dataset in exp_dict[\"datasets\"]:\n",
    "                self.all_datasets.append(dataset[\"name\"])\n",
    "        self.all_datasets = set(self.all_datasets)\n",
    "\n",
    "        for pos_dict in self.pos_info:\n",
    "            self.training[\"expdata\"].append(pos_dict[\"expdata\"])\n",
    "            self.training[\"posdatasets\"].append(pos_dict[\"name\"])\n",
    "            self.validation[\"expdata\"].append(pos_dict[\"expdata\"])\n",
    "            self.validation[\"posdatasets\"].append(pos_dict[\"name\"])\n",
    "        if self.integ_info is not None:\n",
    "            for integ_dict in self.integ_info:\n",
    "                self.training[\"expdata\"].append(integ_dict[\"expdata\"])\n",
    "                self.training[\"integdatasets\"].append(integ_dict[\"name\"])\n",
    "\n",
    "    def _model_generation(self, pdf_models, partition, partition_idx):\n",
    "       \n",
    "        log.info(\"Generating the Model\")\n",
    "        input_arr = np.concatenate(self.input_list, axis=1).T\n",
    "        if self._scaler:\n",
    "            input_arr = self._scaler(input_arr)\n",
    "        input_layer = op.numpy_to_input(input_arr)\n",
    "\n",
    "        all_replicas_pdf = []\n",
    "        for pdf_model in pdf_models:\n",
    "            full_model_input_dict, full_pdf = pdf_model.apply_as_layer([input_layer])\n",
    "\n",
    "            all_replicas_pdf.append(full_pdf)\n",
    "\n",
    "        full_pdf_per_replica = op.stack(all_replicas_pdf, axis=-1)\n",
    "\n",
    "        sp_ar = [self.input_sizes]\n",
    "        sp_kw = {\"axis\": 1}\n",
    "        splitting_layer = op.as_layer(op.split, op_args=sp_ar, op_kwargs=sp_kw, name=\"pdf_split\")\n",
    "        splitted_pdf = splitting_layer(full_pdf_per_replica)\n",
    "\n",
    "        training_mask = validation_mask = experimental_mask = [None]\n",
    "        if partition and partition[\"datasets\"]:\n",
    "            if partition.get(\"overfit\", False):\n",
    "                training_mask = [i[partition_idx] for i in self.training[\"folds\"]]\n",
    "                validation_mask = [i[partition_idx] for i in self.validation[\"folds\"]]\n",
    "            experimental_mask = [i[partition_idx] for i in self.experimental[\"folds\"]]\n",
    "        output_tr = _pdf_injection(splitted_pdf, self.training[\"output\"], training_mask)\n",
    "        training = MetaModel(full_model_input_dict, output_tr)\n",
    "\n",
    "        val_pdfs = []\n",
    "        exp_pdfs = []\n",
    "        for partial_pdf, obs in zip(splitted_pdf, self.training[\"output\"]):\n",
    "            if not obs.positivity and not obs.integrability:\n",
    "                val_pdfs.append(partial_pdf)\n",
    "                exp_pdfs.append(partial_pdf)\n",
    "            elif not obs.integrability and obs.positivity:\n",
    "                val_pdfs.append(partial_pdf)\n",
    "\n",
    "        output_vl = _pdf_injection(val_pdfs, self.validation[\"output\"], validation_mask)\n",
    "        validation = MetaModel(full_model_input_dict, output_vl)\n",
    "\n",
    "        output_ex = _pdf_injection(exp_pdfs, self.experimental[\"output\"], experimental_mask)\n",
    "        experimental = MetaModel(full_model_input_dict, output_ex)\n",
    "\n",
    "        if self.print_summary:\n",
    "            training.summary()\n",
    "\n",
    "        models = {\n",
    "            \"training\": training,\n",
    "            \"validation\": validation,\n",
    "            \"experimental\": experimental,\n",
    "        }\n",
    "\n",
    "        return models\n",
    "\n",
    "    def _reset_observables(self):\n",
    "        \n",
    "        self.input_list = []\n",
    "        self.input_sizes = []\n",
    "        for key in [\"output\", \"posmultipliers\", \"integmultipliers\"]:\n",
    "            self.training[key] = []\n",
    "            self.validation[key] = []\n",
    "            self.experimental[key] = []\n",
    "\n",
    "    def _generate_observables(\n",
    "        self,\n",
    "        all_pos_multiplier,\n",
    "        all_pos_initial,\n",
    "        all_integ_multiplier,\n",
    "        all_integ_initial,\n",
    "        epochs,\n",
    "        interpolation_points,\n",
    "    ):\n",
    "        \n",
    "        self._reset_observables()\n",
    "        log.info(\"Generating layers\")\n",
    "        for exp_dict in self.exp_info:\n",
    "            if not self.mode_hyperopt:\n",
    "                log.info(\"Generating layers for experiment %s\", exp_dict[\"name\"])\n",
    "\n",
    "            exp_layer = observable_generator(exp_dict)\n",
    "\n",
    "            self.input_list += exp_layer[\"inputs\"]\n",
    "            self.input_sizes.append(exp_layer[\"experiment_xsize\"])\n",
    "\n",
    "            self.training[\"output\"].append(exp_layer[\"output_tr\"])\n",
    "            self.validation[\"output\"].append(exp_layer[\"output_vl\"])\n",
    "            self.experimental[\"output\"].append(exp_layer[\"output\"])\n",
    "\n",
    "        for pos_dict in self.pos_info:\n",
    "            if not self.mode_hyperopt:\n",
    "                log.info(\"Generating positivity penalty for %s\", pos_dict[\"name\"])\n",
    "\n",
    "            positivity_steps = int(epochs / PUSH_POSITIVITY_EACH)\n",
    "            max_lambda = pos_dict[\"lambda\"]\n",
    "\n",
    "            pos_initial, pos_multiplier = _LM_initial_and_multiplier(\n",
    "                all_pos_initial, all_pos_multiplier, max_lambda, positivity_steps\n",
    "            )\n",
    "\n",
    "            pos_layer = observable_generator(pos_dict, positivity_initial=pos_initial)\n",
    "            self.input_list += pos_layer[\"inputs\"]\n",
    "            self.input_sizes.append(pos_layer[\"experiment_xsize\"])\n",
    "\n",
    "            self.training[\"output\"].append(pos_layer[\"output_tr\"])\n",
    "            self.validation[\"output\"].append(pos_layer[\"output_tr\"])\n",
    "\n",
    "            self.training[\"posmultipliers\"].append(pos_multiplier)\n",
    "            self.training[\"posinitials\"].append(pos_initial)\n",
    "\n",
    "        if self.integ_info is not None:\n",
    "            for integ_dict in self.integ_info:\n",
    "                if not self.mode_hyperopt:\n",
    "                    log.info(\"Generating integrability penalty for %s\", integ_dict[\"name\"])\n",
    "\n",
    "                integrability_steps = int(epochs / PUSH_INTEGRABILITY_EACH)\n",
    "                max_lambda = integ_dict[\"lambda\"]\n",
    "\n",
    "                integ_initial, integ_multiplier = _LM_initial_and_multiplier(\n",
    "                    all_integ_initial, all_integ_multiplier, max_lambda, integrability_steps\n",
    "                )\n",
    "\n",
    "                integ_layer = observable_generator(\n",
    "                    integ_dict, positivity_initial=integ_initial, integrability=True\n",
    "                )\n",
    "                self.input_list += integ_layer[\"inputs\"]\n",
    "                self.input_sizes.append(integ_layer[\"experiment_xsize\"])\n",
    "                self.training[\"output\"].append(integ_layer[\"output_tr\"])\n",
    "                self.training[\"integmultipliers\"].append(integ_multiplier)\n",
    "                self.training[\"integinitials\"].append(integ_initial)\n",
    "\n",
    "        if interpolation_points:\n",
    "            input_arr = np.concatenate(self.input_list, axis=1)\n",
    "            input_arr = np.sort(input_arr)\n",
    "            input_arr_size = input_arr.size\n",
    "\n",
    "            force_set_smallest = input_arr.min() > 1e-9\n",
    "            if force_set_smallest:\n",
    "                new_xgrid = np.linspace(\n",
    "                    start=1/input_arr_size, stop=1.0, endpoint=False, num=input_arr_size\n",
    "                )\n",
    "            else:\n",
    "                new_xgrid = np.linspace(start=0, stop=1.0, endpoint=False, num=input_arr_size)\n",
    "\n",
    "            unique, counts = np.unique(input_arr, return_counts=True)\n",
    "            map_to_complete = []\n",
    "            for cumsum_ in np.cumsum(counts):\n",
    "                map_to_complete.append(new_xgrid[cumsum_ - counts[0]])\n",
    "            map_to_complete = np.array(map_to_complete)\n",
    "            map_from_complete = unique\n",
    "\n",
    "            if force_set_smallest:\n",
    "                map_from_complete = np.insert(map_from_complete, 0, 1e-9)\n",
    "                map_to_complete = np.insert(map_to_complete, 0, 0.0)\n",
    "\n",
    "            onein = map_from_complete.size / (int(interpolation_points) - 1)\n",
    "            selected_points = [round(i * onein - 1) for i in range(1, int(interpolation_points))]\n",
    "            if selected_points[0] != 0:\n",
    "                selected_points = [0] + selected_points\n",
    "            map_from = map_from_complete[selected_points]\n",
    "            map_from = np.log(map_from)\n",
    "            map_to = map_to_complete[selected_points]\n",
    "\n",
    "            try:\n",
    "                scaler = PchipInterpolator(map_from, map_to)\n",
    "            except ValueError:\n",
    "                raise ValueError(\n",
    "                    \"interpolation_points is larger than the number of unique \"\n",
    "                                    \"input x-values\"\n",
    "                )\n",
    "            self._scaler = lambda x: np.concatenate([scaler(np.log(x)), x], axis=-1)\n",
    "\n",
    "    def _generate_pdf(\n",
    "        self,\n",
    "        nodes_per_layer,\n",
    "        activation_per_layer,\n",
    "        initializer,\n",
    "        layer_type,\n",
    "        dropout,\n",
    "        regularizer,\n",
    "        regularizer_args,\n",
    "        seed,\n",
    "    ):\n",
    "        log.info(\"Generating PDF models\")\n",
    "\n",
    "        pdf_models = pdfNN_layer_generator(\n",
    "            nodes=nodes_per_layer,\n",
    "            activations=activation_per_layer,\n",
    "            As_number=len(LIST_OF_FITTED_A),\n",
    "            layer_type=layer_type,\n",
    "            flav_info=self.flavinfo,\n",
    "            fitbasis=self.fitbasis,\n",
    "            seed=seed,\n",
    "            initializer_name=initializer,\n",
    "            dropout=dropout,\n",
    "            regularizer=regularizer,\n",
    "            regularizer_args=regularizer_args,\n",
    "            impose_sumrule=self.impose_sumrule,\n",
    "            scaler=self._scaler,\n",
    "            parallel_models=self._parallel_models,\n",
    "        )\n",
    "        return pdf_models\n",
    "\n",
    "    def _prepare_reporting(self, partition):\n",
    "\n",
    "        reported_keys = [\"name\", \"count_chi2\", \"positivity\", \"integrability\", \"ndata\", \"ndata_vl\"]\n",
    "        reporting_list = []\n",
    "        for exp_dict in self.all_info:\n",
    "            reporting_dict = {k: exp_dict.get(k) for k in reported_keys}\n",
    "            if partition:\n",
    "                for dataset in exp_dict[\"datasets\"]:\n",
    "                    if dataset in partition[\"datasets\"]:\n",
    "                        ndata = dataset[\"ndata\"]\n",
    "                        frac = dataset[\"frac\"]\n",
    "                        reporting_dict[\"ndata\"] -= int(ndata * frac)\n",
    "                        reporting_dict[\"ndata_vl\"] = int(ndata * (1 - frac))\n",
    "            reporting_list.append(reporting_dict)\n",
    "        return reporting_list\n",
    "\n",
    "    def _train_and_fit(self, training_model, stopping_object, epochs=100):\n",
    "        \n",
    "        callback_st = callbacks.StoppingCallback(stopping_object)\n",
    "        callback_pos = callbacks.LagrangeCallback(\n",
    "            self.training[\"posdatasets\"],\n",
    "            self.training[\"posmultipliers\"],\n",
    "            update_freq=PUSH_POSITIVITY_EACH,\n",
    "        )\n",
    "        callback_integ = callbacks.LagrangeCallback(\n",
    "            self.training[\"integdatasets\"],\n",
    "            self.training[\"integmultipliers\"],\n",
    "            update_freq=PUSH_INTEGRABILITY_EACH,\n",
    "        )\n",
    "\n",
    "        training_model.perform_fit(\n",
    "            epochs=epochs,\n",
    "            verbose=False,\n",
    "            callbacks=self.callbacks + [callback_st, callback_pos, callback_integ],\n",
    "        )\n",
    "\n",
    "        if any(bool(i) for i in stopping_object.e_best_chi2):\n",
    "            return self.pass_status\n",
    "        return self.failed_status\n",
    "\n",
    "    def _hyperopt_override(self, params):\n",
    "        \n",
    "        hyperparameters = params.get(\"parameters\")\n",
    "        if hyperparameters is not None:\n",
    "            return hyperparameters\n",
    "        for hyperkey in self._hyperkeys:\n",
    "            item = params[hyperkey]\n",
    "            if isinstance(item, dict):\n",
    "                params.update(item)\n",
    "        return params\n",
    "\n",
    "    def enable_tensorboard(self, logdir, weight_freq=0, profiling=False):\n",
    "        \n",
    "        callback_tb = callbacks.gen_tensorboard_callback(\n",
    "            logdir, profiling=profiling, histogram_freq=weight_freq\n",
    "        )\n",
    "        self.callbacks.append(callback_tb)\n",
    "\n",
    "    def evaluate(self, stopping_object):\n",
    "        \n",
    "        if self.training[\"model\"] is None:\n",
    "            raise RuntimeError(\"Modeltrainer.evaluate was called before any training\")\n",
    "        train_chi2 = stopping_object.evaluate_training(self.training[\"model\"])\n",
    "        val_chi2 = stopping_object.vl_chi2\n",
    "        exp_chi2 = self.experimental[\"model\"].compute_losses()[\"loss\"] / self.experimental[\"ndata\"]\n",
    "        return train_chi2, val_chi2, exp_chi2\n",
    "\n",
    "    def hyperparametrizable(self, params):\n",
    "        \n",
    "        clear_backend_state()\n",
    "\n",
    "        if self.mode_hyperopt:\n",
    "            log.info(\"Performing hyperparameter scan\")\n",
    "            for key in self._hyperkeys:\n",
    "                log.info(\" > > Testing %s = %s\", key, params[key])\n",
    "            params = self._hyperopt_override(params)\n",
    "\n",
    "        epochs = int(params[\"epochs\"])\n",
    "        stopping_patience = params[\"stopping_patience\"]\n",
    "        stopping_epochs = int(epochs * stopping_patience)\n",
    "\n",
    "        positivity_dict = params.get(\"positivity\", {})\n",
    "        integrability_dict = params.get(\"integrability\", {})\n",
    "        self._generate_observables(\n",
    "            positivity_dict.get(\"multiplier\"),\n",
    "            positivity_dict.get(\"initial\"),\n",
    "            integrability_dict.get(\"multiplier\"),\n",
    "            integrability_dict.get(\"initial\"),\n",
    "            epochs,\n",
    "            params.get(\"interpolation_points\"),\n",
    "        )\n",
    "        threshold_pos = positivity_dict.get(\"threshold\", 1e-6)\n",
    "        threshold_chi2 = params.get(\"threshold_chi2\", CHI2_THRESHOLD)\n",
    "\n",
    "        l_valid = []\n",
    "        l_exper = []\n",
    "        l_hyper = []\n",
    "\n",
    "        n3pdfs = []\n",
    "        exp_models = []\n",
    "\n",
    "        for k, partition in enumerate(self.kpartitions):\n",
    "            seeds = self._nn_seeds\n",
    "            if k > 0:\n",
    "                seeds = [np.random.randint(0, pow(2, 31)) for _ in seeds]\n",
    "\n",
    "            pdf_models = self._generate_pdf(\n",
    "                params[\"nodes_per_layer\"],\n",
    "                params[\"activation_per_layer\"],\n",
    "                params[\"initializer\"],\n",
    "                params[\"layer_type\"],\n",
    "                params[\"dropout\"],\n",
    "                params.get(\"regularizer\", None),\n",
    "                params.get(\"regularizer_args\", None),\n",
    "                seeds,\n",
    "            )\n",
    "\n",
    "            models = self._model_generation(pdf_models, partition, k)\n",
    "\n",
    "            if self.model_file:\n",
    "                log.info(\"Applying model file %s\", self.model_file)\n",
    "                for pdf_model in pdf_models:\n",
    "                    pdf_model.load_weights(self.model_file)\n",
    "\n",
    "            if k > 0:\n",
    "                pos_and_int = self.training[\"posdatasets\"] + self.training[\"integdatasets\"]\n",
    "                initial_values = self.training[\"posinitials\"] + self.training[\"posinitials\"]\n",
    "                models[\"training\"].reset_layer_weights_to(pos_and_int, initial_values)\n",
    "\n",
    "            reporting = self._prepare_reporting(partition)\n",
    "\n",
    "            if self.no_validation:\n",
    "                models[\"validation\"] = models[\"training\"]\n",
    "                validation_model = models[\"training\"]\n",
    "            else:\n",
    "                validation_model = models[\"validation\"]\n",
    "\n",
    "            stopping_object = Stopping(\n",
    "                validation_model,\n",
    "                reporting,\n",
    "                pdf_models,\n",
    "                total_epochs=epochs,\n",
    "                stopping_patience=stopping_epochs,\n",
    "                threshold_positivity=threshold_pos,\n",
    "                threshold_chi2=threshold_chi2,\n",
    "            )\n",
    "\n",
    "            for model in models.values():\n",
    "                model.compile(**params[\"optimizer\"])\n",
    "\n",
    "            passed = self._train_and_fit(\n",
    "                models[\"training\"],\n",
    "                stopping_object,\n",
    "                epochs=epochs,\n",
    "            )\n",
    "\n",
    "            if self.mode_hyperopt:\n",
    "                validation_loss = np.mean(stopping_object.vl_chi2)\n",
    "\n",
    "                exp_loss_raw = np.average(models[\"experimental\"].compute_losses()[\"loss\"])\n",
    "                ndata = np.sum([np.count_nonzero(i[k]) for i in self.experimental[\"folds\"]])\n",
    "                if ndata == 0:\n",
    "                    ndata = self.experimental[\"ndata\"]\n",
    "                experimental_loss = exp_loss_raw / ndata\n",
    "\n",
    "                hyper_loss = experimental_loss\n",
    "                if passed != self.pass_status:\n",
    "                    log.info(\"Hyperparameter combination fail to find a good fit, breaking\")\n",
    "                    break\n",
    "                for penalty in self.hyper_penalties:\n",
    "                    hyper_loss += penalty(pdf_models=pdf_models, stopping_object=stopping_object)\n",
    "                log.info(\"Fold %d finished, loss=%.1f, pass=%s\", k + 1, hyper_loss, passed)\n",
    "\n",
    "                l_hyper.append(hyper_loss)\n",
    "                l_valid.append(validation_loss)\n",
    "                l_exper.append(experimental_loss)\n",
    "                n3pdfs.append(N3PDF(pdf_models, name=f\"fold_{k}\"))\n",
    "                exp_models.append(models[\"experimental\"])\n",
    "\n",
    "                if hyper_loss > self.hyper_threshold:\n",
    "                    log.info(\n",
    "                        \"Loss above threshold (%.1f > %.1f), breaking\",\n",
    "                        hyper_loss,\n",
    "                        self.hyper_threshold,\n",
    "                    )\n",
    "                    pen_mul = len(self.kpartitions) - k\n",
    "                    l_hyper = [i * pen_mul for i in l_hyper]\n",
    "                    break\n",
    "\n",
    "        if self.mode_hyperopt:\n",
    "            dict_out = {\n",
    "                \"status\": passed,\n",
    "                \"loss\": self._hyper_loss(fold_losses=l_hyper, n3pdfs=n3pdfs, experimental_models=exp_models),\n",
    "                \"validation_loss\": np.average(l_valid),\n",
    "                \"experimental_loss\": np.average(l_exper),\n",
    "                \"kfold_meta\": {\n",
    "                    \"validation_losses\": l_valid,\n",
    "                    \"experimental_losses\": l_exper,\n",
    "                    \"hyper_losses\": l_hyper,\n",
    "                },\n",
    "            }\n",
    "            return dict_out\n",
    "\n",
    "        self.training[\"model\"] = models[\"training\"]\n",
    "        self.experimental[\"model\"] = models[\"experimental\"]\n",
    "        self.validation[\"model\"] = models[\"validation\"]\n",
    "        dict_out = {\"status\": passed, \"stopping_object\": stopping_object, \"pdf_models\": pdf_models}\n",
    "        return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62928bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarizes_values(modeltrain, value_name):\n",
    "    \"\"\"Gives a brief Summary.\"\"\"\n",
    "    \n",
    "    value = getattr(modeltrain, value_name)\n",
    "    table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(f\"Keys ({value_name})\", justify=\"left\", width=24)\n",
    "    table.add_column(\"Description\", justify=\"left\", width=24)\n",
    "    table.add_column(\"Value\", justify=\"left\", width=24)\n",
    "    \n",
    "    table.add_row(\"model\", \"Model\", f\"{value['model'] if value['model'] is not None else 'None'}\")\n",
    "    table.add_row(\"expdata\", \"Nb. Experiments\", f\"{len(value['expdata'])}\")\n",
    "    table.add_row(\"output\", \"Nb. Outputs\", f\"{len(value['output'])}\")\n",
    "    table.add_row(\"ndata\", \"Nb. Datapoints\", f\"{value['ndata']}\")\n",
    "    \n",
    "    console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82983506",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'nodes_per_layer': [15, 10, 8], \n",
    "    'activation_per_layer': ['sigmoid', 'sigmoid', 'linear'], \n",
    "    'initializer': 'glorot_normal', \n",
    "    'optimizer': {'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'clipnorm': 1.0}, \n",
    "    'epochs': 900, 'positivity': {'multiplier': 1.05, 'initial': None, 'threshold': 1e-05}, \n",
    "    'stopping_patience': 0.3, 'layer_type': 'dense', 'dropout': 0.0, 'threshold_chi2': 5.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0fc3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "flav_info = [\n",
    "    {'fl': 'sng', 'trainable': False, 'smallx': [1.094, 1.118], 'largex': [1.46, 3.003]}, \n",
    "    {'fl': 'g', 'trainable': False, 'smallx': [0.8189, 1.044], 'largex': [2.791, 5.697]}, \n",
    "    {'fl': 'v', 'trainable': False, 'smallx': [0.457, 0.7326], 'largex': [1.56, 3.431]}, \n",
    "    {'fl': 'v3', 'trainable': False, 'smallx': [0.1462, 0.4061], 'largex': [1.745, 3.452]}, \n",
    "    {'fl': 'v8', 'trainable': False, 'smallx': [0.5401, 0.7665], 'largex': [1.539, 3.393]}, \n",
    "    {'fl': 't3', 'trainable': False, 'smallx': [-0.4401, 0.9163], 'largex': [1.773, 3.333]}, \n",
    "    {'fl': 't8', 'trainable': False, 'smallx': [0.5852, 0.8537], 'largex': [1.533, 3.436]}, \n",
    "    {'fl': 't15', 'trainable': False, 'smallx': [1.082, 1.142], 'largex': [1.461, 3.1]}\n",
    "]\n",
    "\n",
    "nnseed = [1872583848]\n",
    "fitbasis = 'EVOL'\n",
    "debug = False\n",
    "max_cores = 8\n",
    "model_file = None\n",
    "sum_rules = False\n",
    "paralle_models = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a2e270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"meta_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(1, 600, 1)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "PDF_0 (MetaModel)               (1, None, 56)        621         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(1, 600, 56, 1)]    0           PDF_0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pdf_split (Lambda)              [(1, 100, 56, 1), (1 0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "DEUTERON_split (Lambda)         [(1, 50, 56, 1), (1, 0           pdf_split[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "NUCLEAR_split (Lambda)          [(1, 50, 56, 1), (1, 0           pdf_split[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "dat_NMCPD_dw (DIS)              (1, 1, 60)           0           DEUTERON_split[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dat_SLACP_dwsh (DIS)            (1, 1, 19)           0           DEUTERON_split[0][1]             \n",
      "__________________________________________________________________________________________________\n",
      "dat_NMC_PB_C (DIS)              (1, 1, 7)            0           NUCLEAR_split[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dat_NMC_BE_C (DIS)              (1, 1, 8)            0           NUCLEAR_split[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dat_CMS_pPb_2JET_5TEV (DY)      (1, 1, 42)           0           NUCLEAR_split[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "dat_CMS_pPb_WM_8TEV (DY)        (1, 1, 12)           0           NUCLEAR_split[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "dat_CMS_1JET_8TEV (DY)          (1, 1, 92)           0           pdf_split[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "dat_POSF2U (DIS)                (1, 1, 20)           0           pdf_split[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "dat_POSFLL (DIS)                (1, 1, 20)           0           pdf_split[0][4]                  \n",
      "__________________________________________________________________________________________________\n",
      "dat_INTEGXT3 (DIS)              (1, 1, 1)            0           pdf_split[0][5]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ConcatV2 (TensorFlo [(1, 1, 79)]         0           dat_NMCPD_dw[0][0]               \n",
      "                                                                 dat_SLACP_dwsh[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ConcatV2_1 (TensorF [(1, 1, 69)]         0           dat_NMC_PB_C[0][0]               \n",
      "                                                                 dat_NMC_BE_C[0][0]               \n",
      "                                                                 dat_CMS_pPb_2JET_5TEV[0][0]      \n",
      "                                                                 dat_CMS_pPb_WM_8TEV[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity (TensorFlo [(1, 1, 92)]         0           dat_CMS_1JET_8TEV[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_1 (TensorF [(1, 1, 20)]         0           dat_POSF2U[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_2 (TensorF [(1, 1, 20)]         0           dat_POSFLL[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_3 (TensorF [(1, 1, 1)]          0           dat_INTEGXT3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "DEUTERON (LossInvcovmat)        (1,)                 6320        tf_op_layer_ConcatV2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "NUCLEAR (LossInvcovmat)         (1,)                 4830        tf_op_layer_ConcatV2_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "CMS (LossInvcovmat)             (1,)                 8556        tf_op_layer_Identity[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "POSF2U (LossPositivity)         (1,)                 1           tf_op_layer_Identity_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "POSFLL (LossPositivity)         (1,)                 1           tf_op_layer_Identity_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "INTEGXT3 (LossIntegrability)    (1,)                 1           tf_op_layer_Identity_3[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 20,330\n",
      "Trainable params: 557\n",
      "Non-trainable params: 19,773\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok',\n",
       " 'stopping_object': <n3fit.stopping.Stopping at 0x7f7d7c2198b0>,\n",
       " 'pdf_models': [<n3fit.backends.keras_backend.MetaModel.MetaModel at 0x7f7d7c03e2e0>]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelTraining = ModelTrainer(\n",
    "    toyexpinfo,\n",
    "    toyposdatasets,\n",
    "    toyintegdatasets,\n",
    "    flav_info,\n",
    "    fitbasis,\n",
    "    nnseed,\n",
    "    debug=debug,\n",
    "    kfold_parameters=None,\n",
    "    max_cores=max_cores,\n",
    "    sum_rules=sum_rules,\n",
    "    parallel_models=1,\n",
    ")\n",
    "\n",
    "pdf_gen_and_train_function = ModelTraining.hyperparametrizable\n",
    "\n",
    "ModelTraining.set_hyperopt(False)\n",
    "\n",
    "pdf_gen_and_train_function(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5ee13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e0105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf40",
   "language": "python",
   "name": "nnpdf40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
