# How to generate APPLgrids

`APPLgrids` are the partonic cross sections needed to compute hadronic observables (proton-proton collisions).
These QCD-perturbative components are generated by `Monte-Carlo` simulation and tabulated in `.root` format.

## APPLgrid generation with MCFM
*Author: Rabah Abdul Khalek*

After installing `MCFM-6.8` interfaced with `mcfm-bridge-0.0.34-nnpdf` (see Installation in~).
Follow the following steps to generate `APPLgrids`.

### Configuration in mcfm-bridge-0.0.34-nnpdf
The first step is to setup the kinematics of the cross-section distribution you're interested in.
To do that, you have to edit `mcfm-bridge-0.0.34-nnpdf/src/mcfm_interface.cxx` by doing the following:

1. Binning:

    In `void book_grid()`, you should dictate the:
    - Type of process: `mcfm-z` for z production, `mcfm-TT` for ttbar, etc.
    - `q2Low`, `q2Up`, `nQ2bins` and `qorder` are binning information for the grid constructor.
    - `Ngrids` the number of grids generated. For example, if your cross-section is double differential in var1(10 bins) and var2(3 bins). You can generate 3 grids corresponding to var2 that each contain 10 bins of var1.
    - `strcpy(gridFiles[0],"_yZ.root");` this dictates the name of the output grid. In our example `LHCBZ13TEV_yZ.root`.
    - `nObsBins[0] = 17;` the number of bins.
    - `static const double _y[18]` the breakdown of binning.
    - `obsBins[0] = { _y };` append the observable.

    **example:**
    ```
        else if ( glabel == "LHCBZ13TEV" )
        {
            std::cout << "LHCb Z -> e+ e- rapidity distribution, 294 pb-1, 13 TeV" << std::endl;
            pdf_function = "mcfm-z";
            q2Low   = 8315.17, q2Up = 8315.19;
            nQ2bins = 3;
            qorder  = 1;

            Ngrids  = 1;
            strcpy(gridFiles[0],"_yZ.root");

            nObsBins[0] = 17;

            static const double _y[18] = { 2.000, 2.125, 2.250, 
                                        2.375, 2.500, 2.625, 
                                        2.750, 2.875, 3.000, 
                                        3.125, 3.250, 3.375, 
                                        3.500, 3.625, 3.750, 
                                        3.875, 4.000, 4.250 };

            obsBins[0] = { _y };

        }
    ```

2. Specify the distribution (here, it's a distribution in rapidity of the outcoming particles 3 and 4):

    In `void getObservable(const double evt[][mxpart])`, you should add the name of your dataset and determine the kinematic variable that the cross-section will span over.
    **example:**
    ```
    else if (glabel == "LHCBZ13TEV")
        {
        Observable [ 0 ] = rapidity34;
        }
    ```

### Configuration of RunCard in MCFM-6.8
For this, you need to duplicate the default run card: `cp MCFM-6.8/Bin/input.DAT MCFM-6.8/Bin/LHCBZ13TEV.dat`.
and edit the later according to the information in the paper, e.g the kinematic cuts, update the masses and theory settings etc.


### Generation
**/!\ To change according to the conda installation via conda /!\ **

You need to `make` again mcfm-bridge after editing `mcfm_interface.cxx`:
```
cd external/mcfm-bridge-0.0.34-nnpdf
make install

cd external/MCFM-6.8
make -j
./runmcfm <InputCard.Dat> #once to create the grids
./runmcfm <InputCard.Dat> #once to fill them
```
### Run MCFM-6.8

## APPLgrid generation with MadGraph5\_aMC@NLO
*Author: Emanuele Roberto Nocera. Edited by Cameron Voisey. 27.01.2020*

This tutorial explains how one can produce APPLgrids using MG5\_aMC (v2.6.3.2) and aMCfast (v2.0.0)

### Step 1 - Install dependencies

A few external codes need to be previously installed (in parenthesis versions that have been tested):

* ROOT (5.34 onwards)
* LHAPDF (6.2.1 onwards)
* APPLgrids (applgridphoton, from external/applgridphoton)
* FastJet (3.3.1 onwards, from external/fastjet-3.3.1)

If you are able to run the nnpdf code, LHAPDF should already be available on you system. If you are able to run apfelcomb, ROOT and APPLgrids should already be available on your system. FastJet can be installed from the external/fastjet-3.3.1 folder in the usual way as

        ./configure --prefix=/fastjet/intallation/path
        make 
        make check
        make install

If you are installing these programs using conda, you can set the installation path by using

        ./configure --prefix=$CONDA_PREFIX

You might need to run

        autoreconf -i

before the usual configuration/build/installation chain.

More information on Fastjet is available at [http://fastjet.fr/](http://fastjet.fr/).

### Step 2 - Install aMCfast

aMCfast [arXiv:1406.7693] can be installed from the external/amcfast-2.0.0 folder in the usual way as

        ./configure --prefix=/amcfast/installation/path
        make
        make install

More information on aMCfast is available at [https://amcfast.hepforge.org/](https://amcfast.hepforge.org/).

### Step 3 - Run MG5\_aMC

MG5_aMC \[[arXiv:1405.0301](https://arxiv.org/abs/1405.0301)\] can be run from the external/MG5_aMC_v2_6_3_2/bin; python 2(.6 or .7) is required. MG5_aMC does not work with python3. Before the first run, please edit the mg5_configuration.txt file in the external/MG5_aMC_v2_6_3_2/input folder. In particular, please make sure to uncomment the following lines:

* text_editor = <your_preferred_text_editor> (e.g. emacs, gedit, ...)
* web_browser = <your_preferred_web_browser> (e.g. firefox, chrome, ...)
* eps_viewer = <your_preferred_eps_viewer> (e.g. gv, ...)
* lhapdf = lhapdf-config
* fastjet = fastjet-config
* applgrid = applgrid-config
* amcfast = amcfast-config

You can now run MG5_aMC. In the external/MG5_aMC_v2_6_3_2/bin folder run

        python2.7 mg5_aMC

We should now generate the process of interest. For example, in the case of W^+ boson production (including NLO QCD corrections), we should type

        generate p p > w+ [QCD]

If one did not want to include NLO QCD corrections and instead only wanted the LO generation, they could type

        generate p p > w+ [LOonly]

Once the process has been generated, we need to output it to some folder. For instance

        output amcfast_test

This will dump the relevant code to run the process into the folder "amcfast_test". Note that the name of the output folder can be omitted. In this case the code chooses some default name, typically PROC*. If you are using MG5_aMC for the first time, you will receive the following message

        Which one do you want to install? (this needs to be done only once)
        1. cuttools  (OPP) [0711.3596]   : will be installed (required)
        2. iregi     (TIR) [1405.0301]   : will be installed (required)
        3. ninja     (OPP) [1403.1229]   : will be installed (recommended)
        4. collier   (TIR) [1604.06792]  : will be installed (recommended)
        5. golem     (TIR) [0807.0605]   : do not install
        You can:
        -> hit 'enter' to proceed
        -> type a number to cycle its options
        -> enter the following command:
        {tool_name} [install|noinstall|{prefixed_installation_path}]
        If you are unsure about what this question means, just type enter to proceed. [300s to answer]         

Please type *1 [enter] 2 [enter] 3 [enter] 4 [enter] [enter]* and wait. Now we can run the process through

        launch

We will get the following message:

        1. Type of perturbative computation               order = NLO         
        2. No MC@[N]LO matching / event generation  fixed_order = OFF         
        3. Shower the generated events                   shower = HERWIG6     
        4. Decay onshell particles                      madspin = OFF         
        5. Add weights to events for new hypp.         reweight = OFF  
        6. Run MadAnalysis5 on the events generated madanalysis = Not Avail.

This means that by default the code runs in the NLO + parton shower mode. The aMCfast interface works only in the fixed-order mode, therefore we need to deactivate the parton shower. This is easily done by typing *2*. This way we get the message:

        1. Type of perturbative computation               order = NLO         
        2. No MC@[N]LO matching / event generation  fixed_order = ON          
        3. Shower the generated events                   shower = OFF       ⇐  ̶H̶E̶R̶W̶I̶G̶6̶ ̶
        4. Decay onshell particles                      madspin = OFF         
        5. Add weights to events for new hypp.         reweight = OFF         
        6. Run MadAnalysis5 on the events generated madanalysis = Not Avail.

which confirms that we are about to run MadGraph5_aMC@NLO in the fixed-order mode at NLO. Press *[enter]* and go ahead. Now we get this message:

        /------------------------------------------------------------\
        |  1. param      : param_card.dat                            |
        |  2. run        : run_card.dat                              |
        |  3. FO_analyse : FO_analyse_card.dat                       |
        \------------------------------------------------------------/

We first need to edit the parameter card file to make sure that the values of all the physical parameters are correct. Please do so by typing *1*.

We then need to edit the run card, typing *2*. To produce grids we cannot use the MG5_aMC internal PDFs. We use instead LHAPDF. To do so, we just set in the run card:

        lhapdf = pdlabel ! PDF set

We then need to specify the identification number of the PDF member that we want to use for the run. For example, for NNPDF31_nnlo_as_0118

        303400 = lhaid ! if pdlabel=lhapdf, this is the lhapdf number

The identification numbers for other PDF sets can be found at the [LHAPDF website](https://lhapdf.hepforge.org/pdfsets.html). We can now save and close the run card and edit the fixed-order analysis card by typing *3*. Here we have to specify the analysis file that will be used during the run. Assuming to have written an analysis file named "analysis_td_pp_V.f", which is supposed to be in the amcfast_test/FixedOrderAnalysis/folder, we need to set in the fixed-order analysis card:

        FO_ANALYSE = analysis_td_template.o

Of course, the way how the analysis file is written must be consistent with the analysis format specified in the fixed-order analysis card itself. In particular, we can set:

        FO_ANALYSIS_FORMAT = topdrawer        

However, the way how the interpolation grids are filled is independent of the analysis format. Note that in amcfast_test/FixedOrderAnalysis you will be able to find an array of different analysis template cards that are designed for different types of analysis. For example, "analysis_HwU_pp_lplm.f" is in the "histogram with uncertainties" format (hence the "HwU") and it is designed for the analysis of opposite sign charged leptons (hence the "lplm", which stands for "lepton plus lepton minus").

We can finally save and close the fixed-order analysis card and start the run by giving *[enter]*. The run should finish successfully, without the generation of any APPLgrid.

We now need to repeat the procedure enabling the generation of the grids. To do so, we have to run again the code giving:

        launch amcfast_test

A new run starts and the same messages shown above will be displayed. For this second run, the idea is to set up an empty grid that will eventually be filled up. In practice, this is done by doing a preliminary "low statistics" run that allows the code to optimize the interpolation grids based on the particular observables defined in the analysis file. Such an optimization acts on the predefined input grids, trimming them in such a way to exclude the unused grid nodes. The parameters of the input grids (number of nodes of the x-space and Q-space grid, interpolation orders, etc.) before the optimization can be set by the user in the analysis file. To perform this preparatory run, we edit the run card and set:

        1 = iappl ! aMCfast switch (0=OFF, 1=prepare APPLgrids, 2=fill grids)

Since at this stage the interpolation grids are not filled up, there is no need for a high accuracy, thus something like:

        0.01 = req_acc_FO

in the run card is enough. The run should end successfully. An (empty) APPLgrid should be available in amcfast_test/Events/run_02/.

We now need to fill the grid. To do so, we have to run the code again with:

        launch -o

where the option "-o" ensures that the code restarts the run from the (integration) grids generated in the previous run. A new run starts and the same messages shown above will be displayed. For this run, we need to edit the run card and set:

        2 = iappl ! aMCfast switch (0=OFF, 1=prepare APPLgrids, 2=fill grids)

In addition, we might want to increase the accuracy of the integration by setting, for example:

        0.001 = req_acc_FO

This will finally lead to the production of the final interpolation grids which should be found in the "amcfast_test/Events/run_03/" folder. The names of the grids are "aMCfast_obs_0.root", "aMCfast_obs_1.root", "aMCfast_obs_2.root", etc. and there should be as many as the observables defined in the analysis file and the numbering follows the definition order.

You can quit MG5_aMC by typing

        exit

More information on MG5_aMC is available at [https://launchpad.net/mg5amcnlo](https://launchpad.net/mg5amcnlo).
